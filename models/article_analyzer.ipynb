{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-11T18:57:13.374456Z",
     "start_time": "2024-08-11T18:57:13.367306Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\remes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\remes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\remes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:57:37.159498Z",
     "start_time": "2024-08-11T18:57:33.090069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summarizer = pipeline(\"summarization\", truncation=True)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "token_analyzer = SentimentAnalyzer()"
   ],
   "id": "cf281db8ec130ee6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:56:58.697432Z",
     "start_time": "2024-08-11T18:56:58.682895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_sentiment(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def article_summarizer(article):\n",
    "    summary = summarizer(article, max_length=300, min_length=100, do_sample=False)\n",
    "    \n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "def get_companies_names():\n",
    "    db = next(get_db())\n",
    "    try:\n",
    "        stocks = db.query(Stock.title).all()\n",
    "        return stocks\n",
    "    finally:\n",
    "        db.close()"
   ],
   "id": "fa9d259113c36873",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "___Basic version of the algorithm___\n",
    "\n",
    "Analyzes pre-processed text. Processing includes: removing advertisements, removing unnecessary abbreviations and abbreviations, repeating the same thing."
   ],
   "id": "314846d7fd58ab58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T14:58:35.429327Z",
     "start_time": "2024-08-03T14:58:22.762529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "articles_marks = []\n",
    "companies_names = list((i[0] for i in get_companies_names()))\n",
    "\n",
    "for root, dirs, files in os.walk(os.getcwd() + \"/data/stocks\"):\n",
    "    for dir_name, company_name in zip(dirs, companies_names):\n",
    "        folder_path = os.path.join(root, dir_name)\n",
    "        pos_mark = 0\n",
    "        neg_mark = 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            article_file = open(file_path, 'r', encoding=\"utf-8\")\n",
    "            article_text = article_file.read()\n",
    "            sum_article = article_summarizer(article_text)\n",
    "            analyze = analyze_sentiment(sum_article)\n",
    "            if analyze['pos'] > analyze['neg']:\n",
    "                pos_mark += 1\n",
    "            else:\n",
    "                neg_mark += 1\n",
    "        articles_marks.append({'stock': company_name, 'positives': pos_mark, 'negatives': neg_mark})\n",
    "        break\n",
    "base_articles_df = pd.DataFrame(articles_marks, columns=['stock', 'positives', 'negatives'])\n",
    "print(base_articles_df)"
   ],
   "id": "b75195ce5d61f6cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 300, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Your max_length is set to 300, but your input_length is only 230. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=115)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                stock  positives  negatives\n",
      "0  NVIDIA Corporation          3          0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Adding news sentiment scores for each stock to the database.__",
   "id": "57bee4e452609290"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:49:02.011329Z",
     "start_time": "2024-08-11T18:49:01.840718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sqlalchemy import text\n",
    "from database.models import Stock, StockSentiment\n",
    "from database.db import get_db\n",
    "\n",
    "def add_to_db(df):\n",
    "    db = next(get_db())\n",
    "    try:\n",
    "        db.query(StockSentiment).delete()\n",
    "        db.commit()\n",
    "\n",
    "        db.execute(text(\"ALTER SEQUENCE stock_sentiments_id_seq RESTART WITH 1\"))\n",
    "        db.commit()\n",
    "        \n",
    "        print(\"Updating stock sentiments...\")\n",
    "\n",
    "        stock_sentiments = []\n",
    "        for index, row in df.iterrows():\n",
    "            stock = db.query(Stock).filter_by(title=row['stock']).first()\n",
    "            if stock:\n",
    "                stock_sentiment = StockSentiment(\n",
    "                    stock_id=stock.id,\n",
    "                    positives=row['positives'],\n",
    "                    negatives=row['negatives']\n",
    "                )\n",
    "                stock_sentiments.append(stock_sentiment)\n",
    "            else:\n",
    "                print(f\"Stock '{row['stock']}' not found in the Stock table\")\n",
    "\n",
    "        db.bulk_save_objects(stock_sentiments)\n",
    "        db.commit()\n",
    "\n",
    "        print(\"Stock sentiments updated successfully.\")\n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        print(f\"Failed to update stock sentiments: {e}\")\n",
    "    finally:\n",
    "        db.close()\n",
    "        \n",
    "    \n",
    "def clear_db():\n",
    "    db = next(get_db())\n",
    "    try:\n",
    "        db.query(StockSentiment).delete()\n",
    "        db.commit()\n",
    "        print(\"Database cleared successfully.\")\n",
    "    except Exception as e:\n",
    "        db.rollback()\n",
    "        print(f\"Failed to update stock sentiments: {e}\")\n",
    "    finally:\n",
    "        db.close()"
   ],
   "id": "19e1d2d3feaa16ae",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T15:12:00.024685Z",
     "start_time": "2024-07-30T15:11:59.790703Z"
    }
   },
   "cell_type": "code",
   "source": "add_to_db(base_articles_df)",
   "id": "f46207ea934289f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating stock sentiments...\n",
      "Stock sentiments updated successfully.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The second version of the algorithm using tokenization.",
   "id": "75d2015e2b80fcdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:57:45.835797Z",
     "start_time": "2024-08-11T18:57:42.798305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "articles_marks = []\n",
    "companies_names = list((i[0] for i in get_companies_names()))\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "articles_list = []\n",
    "\n",
    "def delete_punctuation(article):\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    article = re.sub(pattern,'',article)\n",
    "    return article\n",
    "\n",
    "\n",
    "def sentence_tokenization(sentence):\n",
    "    article_word_list = nltk.tokenize.word_tokenize(sentence)\n",
    "    return article_word_list\n",
    "\n",
    "\n",
    "def article_tokenization(article):\n",
    "    sentences = nltk.tokenize.sent_tokenize(article)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def delete_stop_words(article_list):\n",
    "    global stop_words\n",
    "    processed_article = [word for word in article_list if word.lower() not in stop_words]\n",
    "    return processed_article\n",
    "\n",
    "\n",
    "def analyze_tokenized_sentiment(tokenized_article):\n",
    "    sentiment_scores = [token_analyzer.all_words(word) for word in tokenized_article if word.isalpha()]\n",
    "    return sentiment_scores\n",
    "\n",
    "for root, dirs, files in os.walk(os.getcwd() + \"/data/stocks\"):\n",
    "    for dir_name, company_name in zip(dirs, companies_names):\n",
    "        folder_path = os.path.join(root, dir_name)\n",
    "        pos_mark = 0\n",
    "        neg_mark = 0\n",
    "        articles_list = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            words_list = []\n",
    "            sentences_list = []\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            article_file = open(file_path, 'r', encoding=\"utf-8\")\n",
    "            article_text = article_file.read()\n",
    "            article = re.sub(r'\\.(?!\\s)', '. ', article_text)\n",
    "            sentences = article_tokenization(article_text)\n",
    "            for sentence in sentences:\n",
    "                sentence = delete_punctuation(sentence)\n",
    "                tokenized_sentence = sentence_tokenization(sentence)\n",
    "                without_base_sw = delete_stop_words(tokenized_sentence)\n",
    "                words_list.append(without_base_sw)\n",
    "            for word_list in words_list:\n",
    "                sentence = ' '.join(word_list)\n",
    "                \n",
    "                sentence = sentence.capitalize()\n",
    "                \n",
    "                sentence = re.sub(r'([.!?])(\\w)', r'\\1 \\2', sentence)\n",
    "                sentences_list.append(sentence)\n",
    "            articles_list.append(sentences_list)\n",
    "        pos_counter = 0 \n",
    "        neg_counter = 0\n",
    "        for article in articles_list:\n",
    "            compound_counter: float = 0\n",
    "            for sentence in article:\n",
    "                if analyze_sentiment(sentence)[\"neg\"] > analyze_sentiment(sentence)[\"pos\"]:\n",
    "                    neg_counter += 1\n",
    "                elif analyze_sentiment(sentence)[\"pos\"] > analyze_sentiment(sentence)[\"neg\"]:\n",
    "                    pos_counter += 1\n",
    "        articles_marks.append({'stock': company_name, 'positives': pos_counter, 'negatives': neg_counter})\n",
    "    \n",
    "\n",
    "# tokenized_sentences_df = pd.DataFrame(articles_marks, columns=['stock', 'positives', 'negatives'])\n",
    "# tokenized_sentences_df"
   ],
   "id": "756c9d3b299ea5b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                stock  positives  negatives\n",
       "0                  NVIDIA Corporation        154         60\n",
       "1               Palantir Technologies        126         26\n",
       "2                   Intel Corporation        148         66\n",
       "3                  Lumen Technologies        178         44\n",
       "4                               Tesla        141         47\n",
       "5                  Ford Motor Company        113         23\n",
       "6                               Apple         63         12\n",
       "7              Warner Bros. Discovery         93         27\n",
       "8              Advanced Micro Devices        140         24\n",
       "9                    Nu Holdings Ltd.         91          9\n",
       "10                         Amazon.com        103         31\n",
       "11                Banco Bradesco S.A.         88         47\n",
       "12                 Energy Transfer LP         54         18\n",
       "13                     Unity Software         33         10\n",
       "14                          Vale S.A.        119         27\n",
       "15  Algonquin Power & Utilities Corp.        134         26\n",
       "16                       B2Gold Corp.         91         30\n",
       "17                           Alphabet         80         27\n",
       "18                               Snap        151         65\n",
       "19                        Lucid Group         93         37\n",
       "20        Bank of America Corporation        112         38\n",
       "21         Carnival Corporation & plc         40         19\n",
       "22                                NIO        204         20\n",
       "23                             Pfizer        381         71\n",
       "24                  Rivian Automotive         13         10"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>positives</th>\n",
       "      <th>negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NVIDIA Corporation</td>\n",
       "      <td>154</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Palantir Technologies</td>\n",
       "      <td>126</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intel Corporation</td>\n",
       "      <td>148</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lumen Technologies</td>\n",
       "      <td>178</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>141</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ford Motor Company</td>\n",
       "      <td>113</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apple</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Warner Bros. Discovery</td>\n",
       "      <td>93</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "      <td>140</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nu Holdings Ltd.</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>103</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Banco Bradesco S.A.</td>\n",
       "      <td>88</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Energy Transfer LP</td>\n",
       "      <td>54</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Unity Software</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vale S.A.</td>\n",
       "      <td>119</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Algonquin Power &amp; Utilities Corp.</td>\n",
       "      <td>134</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B2Gold Corp.</td>\n",
       "      <td>91</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alphabet</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Snap</td>\n",
       "      <td>151</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lucid Group</td>\n",
       "      <td>93</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bank of America Corporation</td>\n",
       "      <td>112</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carnival Corporation &amp; plc</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NIO</td>\n",
       "      <td>204</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Pfizer</td>\n",
       "      <td>381</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Rivian Automotive</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:57:53.301222Z",
     "start_time": "2024-08-11T18:57:51.171516Z"
    }
   },
   "cell_type": "code",
   "source": "add_to_db(tokenized_sentences_df)",
   "id": "a4f36307125c55d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating stock sentiments...\n",
      "Stock sentiments updated successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db = next(get_db())\n",
    "db.query(StockSentiment).delete()\n",
    "db.commit()"
   ],
   "id": "f5af22cd52b0da62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T17:47:37.538349Z",
     "start_time": "2024-07-24T17:47:35.605916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "articles_marks = []\n",
    "companies_names = list((i[0] for i in get_companies_names()))\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "articles_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(os.getcwd() + \"/data/stocks\"):\n",
    "    for dir_name, company_name in zip(dirs, companies_names):\n",
    "        folder_path = os.path.join(root, dir_name)\n",
    "        pos_mark = 0\n",
    "        neg_mark = 0\n",
    "        articles_list = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            words_list = []\n",
    "            sentences_list = []\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            article_file = open(file_path, 'r', encoding=\"utf-8\")\n",
    "            article_text = article_file.read()\n",
    "            article = re.sub(r'\\.(?!\\s)', '. ', article_text)\n",
    "            sentences = article_tokenization(article_text)\n",
    "            for sentence in sentences:\n",
    "                sentence = delete_punctuation(sentence)\n",
    "                tokenized_sentence = sentence_tokenization(sentence)\n",
    "                without_base_sw = delete_stop_words(tokenized_sentence)\n",
    "                words_list.append(without_base_sw)\n",
    "            for word_list in words_list:\n",
    "                sentence = ' '.join(word_list)\n",
    "                \n",
    "                sentence = sentence.capitalize()\n",
    "                sentence = sentence + \".\"\n",
    "                \n",
    "                sentences_list.append(sentence)\n",
    "            articles_list.append(sentences_list)\n",
    "        pos_counter = 0 \n",
    "        neg_counter = 0\n",
    "        full_articles_list = []\n",
    "        for article in articles_list:\n",
    "            full_article = \"\"\n",
    "            for sentence in article:\n",
    "                full_article = full_article + sentence + \" \"\n",
    "            full_articles_list.append(full_article)\n",
    "            \n",
    "        for article_text in full_articles_list:\n",
    "            if analyze_sentiment(article_text)[\"compound\"] <= 0:\n",
    "                neg_counter += 1\n",
    "            elif analyze_sentiment(article_text)[\"compound\"] > 0:\n",
    "                pos_counter += 1\n",
    "            print(analyze_sentiment(article_text))\n",
    "        articles_marks.append({'stock': company_name, 'positives': pos_counter, 'negatives': neg_counter})\n",
    "        \n",
    "tokenized_articles_df = pd.DataFrame(articles_marks, columns=['stock', 'positives', 'negatives'])\n",
    "tokenized_articles_df"
   ],
   "id": "1fd7a6d207769e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.059, 'neu': 0.859, 'pos': 0.082, 'compound': 0.9186}\n",
      "{'neg': 0.069, 'neu': 0.812, 'pos': 0.119, 'compound': 0.9887}\n",
      "{'neg': 0.051, 'neu': 0.861, 'pos': 0.087, 'compound': 0.8467}\n",
      "{'neg': 0.069, 'neu': 0.817, 'pos': 0.115, 'compound': 0.967}\n",
      "{'neg': 0.028, 'neu': 0.777, 'pos': 0.195, 'compound': 0.9971}\n",
      "{'neg': 0.059, 'neu': 0.832, 'pos': 0.109, 'compound': 0.9363}\n",
      "{'neg': 0.056, 'neu': 0.764, 'pos': 0.18, 'compound': 0.9952}\n",
      "{'neg': 0.028, 'neu': 0.773, 'pos': 0.198, 'compound': 0.9956}\n",
      "{'neg': 0.061, 'neu': 0.777, 'pos': 0.162, 'compound': 0.9851}\n",
      "{'neg': 0.033, 'neu': 0.811, 'pos': 0.156, 'compound': 0.9783}\n",
      "{'neg': 0.01, 'neu': 0.812, 'pos': 0.178, 'compound': 0.9916}\n",
      "{'neg': 0.028, 'neu': 0.773, 'pos': 0.198, 'compound': 0.9956}\n",
      "{'neg': 0.037, 'neu': 0.827, 'pos': 0.136, 'compound': 0.9843}\n",
      "{'neg': 0.053, 'neu': 0.69, 'pos': 0.257, 'compound': 0.9963}\n",
      "{'neg': 0.043, 'neu': 0.787, 'pos': 0.17, 'compound': 0.9966}\n",
      "{'neg': 0.054, 'neu': 0.759, 'pos': 0.187, 'compound': 0.9825}\n",
      "{'neg': 0.028, 'neu': 0.853, 'pos': 0.118, 'compound': 0.9794}\n",
      "{'neg': 0.039, 'neu': 0.822, 'pos': 0.138, 'compound': 0.9948}\n",
      "{'neg': 0.051, 'neu': 0.694, 'pos': 0.254, 'compound': 0.9937}\n",
      "{'neg': 0.038, 'neu': 0.792, 'pos': 0.17, 'compound': 0.9821}\n",
      "{'neg': 0.049, 'neu': 0.79, 'pos': 0.161, 'compound': 0.9716}\n",
      "{'neg': 0.04, 'neu': 0.746, 'pos': 0.214, 'compound': 0.991}\n",
      "{'neg': 0.134, 'neu': 0.674, 'pos': 0.192, 'compound': 0.8126}\n",
      "{'neg': 0.019, 'neu': 0.823, 'pos': 0.158, 'compound': 0.9823}\n",
      "{'neg': 0.049, 'neu': 0.759, 'pos': 0.192, 'compound': 0.9834}\n",
      "{'neg': 0.017, 'neu': 0.771, 'pos': 0.212, 'compound': 0.9984}\n",
      "{'neg': 0.025, 'neu': 0.839, 'pos': 0.135, 'compound': 0.9886}\n",
      "{'neg': 0.059, 'neu': 0.76, 'pos': 0.181, 'compound': 0.9907}\n",
      "{'neg': 0.054, 'neu': 0.745, 'pos': 0.201, 'compound': 0.9958}\n",
      "{'neg': 0.083, 'neu': 0.798, 'pos': 0.119, 'compound': 0.9446}\n",
      "{'neg': 0.047, 'neu': 0.836, 'pos': 0.117, 'compound': 0.9578}\n",
      "{'neg': 0.104, 'neu': 0.78, 'pos': 0.116, 'compound': 0.5638}\n",
      "{'neg': 0.063, 'neu': 0.743, 'pos': 0.193, 'compound': 0.9943}\n",
      "{'neg': 0.025, 'neu': 0.741, 'pos': 0.234, 'compound': 0.9918}\n",
      "{'neg': 0.046, 'neu': 0.84, 'pos': 0.114, 'compound': 0.979}\n",
      "{'neg': 0.061, 'neu': 0.833, 'pos': 0.106, 'compound': 0.9683}\n",
      "{'neg': 0.04, 'neu': 0.722, 'pos': 0.237, 'compound': 0.9967}\n",
      "{'neg': 0.125, 'neu': 0.551, 'pos': 0.324, 'compound': 0.9698}\n",
      "{'neg': 0.027, 'neu': 0.803, 'pos': 0.171, 'compound': 0.9756}\n",
      "{'neg': 0.013, 'neu': 0.762, 'pos': 0.224, 'compound': 0.9986}\n",
      "{'neg': 0.022, 'neu': 0.772, 'pos': 0.206, 'compound': 0.9976}\n",
      "{'neg': 0.059, 'neu': 0.738, 'pos': 0.204, 'compound': 0.9918}\n",
      "{'neg': 0.039, 'neu': 0.765, 'pos': 0.196, 'compound': 0.9972}\n",
      "{'neg': 0.012, 'neu': 0.775, 'pos': 0.213, 'compound': 0.9983}\n",
      "{'neg': 0.046, 'neu': 0.807, 'pos': 0.146, 'compound': 0.9909}\n",
      "{'neg': 0.009, 'neu': 0.815, 'pos': 0.176, 'compound': 0.9911}\n",
      "{'neg': 0.044, 'neu': 0.769, 'pos': 0.187, 'compound': 0.9963}\n",
      "{'neg': 0.092, 'neu': 0.728, 'pos': 0.18, 'compound': 0.9929}\n",
      "{'neg': 0.094, 'neu': 0.715, 'pos': 0.191, 'compound': 0.995}\n",
      "{'neg': 0.037, 'neu': 0.745, 'pos': 0.217, 'compound': 0.9985}\n",
      "{'neg': 0.058, 'neu': 0.75, 'pos': 0.192, 'compound': 0.9918}\n",
      "{'neg': 0.069, 'neu': 0.772, 'pos': 0.159, 'compound': 0.9764}\n",
      "{'neg': 0.047, 'neu': 0.836, 'pos': 0.117, 'compound': 0.9578}\n",
      "{'neg': 0.041, 'neu': 0.769, 'pos': 0.19, 'compound': 0.9974}\n",
      "{'neg': 0.016, 'neu': 0.725, 'pos': 0.259, 'compound': 0.9919}\n",
      "{'neg': 0.052, 'neu': 0.743, 'pos': 0.206, 'compound': 0.9983}\n",
      "{'neg': 0.017, 'neu': 0.809, 'pos': 0.174, 'compound': 0.9895}\n",
      "{'neg': 0.064, 'neu': 0.732, 'pos': 0.204, 'compound': 0.6808}\n",
      "{'neg': 0.04, 'neu': 0.788, 'pos': 0.173, 'compound': 0.9919}\n",
      "{'neg': 0.021, 'neu': 0.859, 'pos': 0.12, 'compound': 0.9886}\n",
      "{'neg': 0.046, 'neu': 0.812, 'pos': 0.141, 'compound': 0.9939}\n",
      "{'neg': 0.035, 'neu': 0.803, 'pos': 0.162, 'compound': 0.9962}\n",
      "{'neg': 0.045, 'neu': 0.806, 'pos': 0.149, 'compound': 0.9985}\n",
      "{'neg': 0.06, 'neu': 0.81, 'pos': 0.13, 'compound': 0.9705}\n",
      "{'neg': 0.039, 'neu': 0.71, 'pos': 0.251, 'compound': 0.9991}\n",
      "{'neg': 0.112, 'neu': 0.746, 'pos': 0.142, 'compound': 0.9042}\n",
      "{'neg': 0.033, 'neu': 0.782, 'pos': 0.185, 'compound': 0.9925}\n",
      "{'neg': 0.027, 'neu': 0.809, 'pos': 0.164, 'compound': 0.9985}\n",
      "{'neg': 0.047, 'neu': 0.741, 'pos': 0.212, 'compound': 0.9984}\n",
      "{'neg': 0.145, 'neu': 0.757, 'pos': 0.097, 'compound': -0.7232}\n",
      "{'neg': 0.118, 'neu': 0.759, 'pos': 0.123, 'compound': 0.7506}\n",
      "{'neg': 0.032, 'neu': 0.782, 'pos': 0.186, 'compound': 0.9976}\n",
      "{'neg': 0.05, 'neu': 0.807, 'pos': 0.144, 'compound': 0.9834}\n",
      "{'neg': 0.016, 'neu': 0.831, 'pos': 0.154, 'compound': 0.9937}\n",
      "{'neg': 0.124, 'neu': 0.757, 'pos': 0.119, 'compound': -0.3818}\n",
      "{'neg': 0.148, 'neu': 0.725, 'pos': 0.127, 'compound': -0.5994}\n",
      "{'neg': 0.016, 'neu': 0.765, 'pos': 0.219, 'compound': 0.9984}\n",
      "{'neg': 0.064, 'neu': 0.782, 'pos': 0.155, 'compound': 0.996}\n",
      "{'neg': 0.053, 'neu': 0.741, 'pos': 0.206, 'compound': 0.9975}\n",
      "{'neg': 0.086, 'neu': 0.757, 'pos': 0.157, 'compound': 0.9821}\n",
      "{'neg': 0.029, 'neu': 0.774, 'pos': 0.196, 'compound': 0.9787}\n",
      "{'neg': 0.045, 'neu': 0.748, 'pos': 0.207, 'compound': 0.9977}\n",
      "{'neg': 0.098, 'neu': 0.753, 'pos': 0.149, 'compound': 0.9903}\n",
      "{'neg': 0.08, 'neu': 0.792, 'pos': 0.129, 'compound': 0.9366}\n",
      "{'neg': 0.046, 'neu': 0.757, 'pos': 0.196, 'compound': 0.998}\n",
      "{'neg': 0.067, 'neu': 0.722, 'pos': 0.211, 'compound': 0.9968}\n",
      "{'neg': 0.032, 'neu': 0.769, 'pos': 0.199, 'compound': 0.9979}\n",
      "{'neg': 0.06, 'neu': 0.77, 'pos': 0.17, 'compound': 0.9953}\n",
      "{'neg': 0.048, 'neu': 0.723, 'pos': 0.229, 'compound': 0.9957}\n",
      "{'neg': 0.025, 'neu': 0.729, 'pos': 0.246, 'compound': 0.9917}\n",
      "{'neg': 0.031, 'neu': 0.806, 'pos': 0.163, 'compound': 0.9958}\n",
      "{'neg': 0.075, 'neu': 0.761, 'pos': 0.164, 'compound': 0.9948}\n",
      "{'neg': 0.033, 'neu': 0.802, 'pos': 0.165, 'compound': 0.9935}\n",
      "{'neg': 0.051, 'neu': 0.766, 'pos': 0.183, 'compound': 0.9959}\n",
      "{'neg': 0.054, 'neu': 0.732, 'pos': 0.215, 'compound': 0.9957}\n",
      "{'neg': 0.056, 'neu': 0.794, 'pos': 0.149, 'compound': 0.9881}\n",
      "{'neg': 0.015, 'neu': 0.843, 'pos': 0.142, 'compound': 0.9792}\n",
      "{'neg': 0.009, 'neu': 0.804, 'pos': 0.187, 'compound': 0.9943}\n",
      "{'neg': 0.041, 'neu': 0.76, 'pos': 0.2, 'compound': 0.9971}\n",
      "{'neg': 0.067, 'neu': 0.819, 'pos': 0.114, 'compound': 0.8689}\n",
      "{'neg': 0.019, 'neu': 0.823, 'pos': 0.158, 'compound': 0.9823}\n",
      "{'neg': 0.009, 'neu': 0.804, 'pos': 0.187, 'compound': 0.9943}\n",
      "{'neg': 0.088, 'neu': 0.709, 'pos': 0.202, 'compound': 0.9867}\n",
      "{'neg': 0.02, 'neu': 0.785, 'pos': 0.195, 'compound': 0.9879}\n",
      "{'neg': 0.026, 'neu': 0.842, 'pos': 0.132, 'compound': 0.9917}\n",
      "{'neg': 0.018, 'neu': 0.756, 'pos': 0.226, 'compound': 0.9964}\n",
      "{'neg': 0.037, 'neu': 0.751, 'pos': 0.212, 'compound': 0.9578}\n",
      "{'neg': 0.078, 'neu': 0.739, 'pos': 0.183, 'compound': 0.9136}\n",
      "{'neg': 0.02, 'neu': 0.786, 'pos': 0.193, 'compound': 0.9982}\n",
      "{'neg': 0.027, 'neu': 0.769, 'pos': 0.205, 'compound': 0.9982}\n",
      "{'neg': 0.048, 'neu': 0.851, 'pos': 0.101, 'compound': 0.9678}\n",
      "{'neg': 0.017, 'neu': 0.811, 'pos': 0.172, 'compound': 0.9945}\n",
      "{'neg': 0.034, 'neu': 0.823, 'pos': 0.143, 'compound': 0.9954}\n",
      "{'neg': 0.014, 'neu': 0.734, 'pos': 0.252, 'compound': 0.9989}\n",
      "{'neg': 0.085, 'neu': 0.745, 'pos': 0.171, 'compound': 0.9622}\n",
      "{'neg': 0.095, 'neu': 0.698, 'pos': 0.207, 'compound': 0.9971}\n",
      "{'neg': 0.018, 'neu': 0.776, 'pos': 0.206, 'compound': 0.9984}\n",
      "{'neg': 0.043, 'neu': 0.843, 'pos': 0.114, 'compound': 0.9643}\n",
      "{'neg': 0.031, 'neu': 0.841, 'pos': 0.128, 'compound': 0.9949}\n",
      "{'neg': 0.032, 'neu': 0.722, 'pos': 0.247, 'compound': 0.9968}\n",
      "{'neg': 0.051, 'neu': 0.889, 'pos': 0.06, 'compound': 0.4767}\n",
      "{'neg': 0.076, 'neu': 0.818, 'pos': 0.107, 'compound': 0.3612}\n",
      "{'neg': 0.046, 'neu': 0.803, 'pos': 0.151, 'compound': 0.9951}\n",
      "{'neg': 0.043, 'neu': 0.884, 'pos': 0.073, 'compound': 0.8658}\n",
      "{'neg': 0.068, 'neu': 0.724, 'pos': 0.208, 'compound': 0.9764}\n",
      "{'neg': 0.025, 'neu': 0.809, 'pos': 0.166, 'compound': 0.967}\n",
      "{'neg': 0.021, 'neu': 0.786, 'pos': 0.193, 'compound': 0.996}\n",
      "{'neg': 0.038, 'neu': 0.895, 'pos': 0.067, 'compound': 0.8225}\n",
      "{'neg': 0.033, 'neu': 0.806, 'pos': 0.161, 'compound': 0.9948}\n",
      "{'neg': 0.063, 'neu': 0.769, 'pos': 0.169, 'compound': 0.9957}\n",
      "{'neg': 0.031, 'neu': 0.913, 'pos': 0.056, 'compound': 0.7579}\n",
      "{'neg': 0.061, 'neu': 0.761, 'pos': 0.178, 'compound': 0.9956}\n",
      "{'neg': 0.044, 'neu': 0.793, 'pos': 0.163, 'compound': 0.9969}\n",
      "{'neg': 0.06, 'neu': 0.775, 'pos': 0.165, 'compound': 0.9956}\n",
      "{'neg': 0.066, 'neu': 0.811, 'pos': 0.123, 'compound': 0.9531}\n",
      "{'neg': 0.064, 'neu': 0.748, 'pos': 0.188, 'compound': 0.9965}\n",
      "{'neg': 0.037, 'neu': 0.85, 'pos': 0.113, 'compound': 0.9709}\n",
      "{'neg': 0.061, 'neu': 0.777, 'pos': 0.162, 'compound': 0.9851}\n",
      "{'neg': 0.068, 'neu': 0.768, 'pos': 0.164, 'compound': 0.9893}\n",
      "{'neg': 0.034, 'neu': 0.756, 'pos': 0.21, 'compound': 0.9942}\n",
      "{'neg': 0.019, 'neu': 0.823, 'pos': 0.158, 'compound': 0.9823}\n",
      "{'neg': 0.033, 'neu': 0.783, 'pos': 0.184, 'compound': 0.9932}\n",
      "{'neg': 0.03, 'neu': 0.697, 'pos': 0.273, 'compound': 0.9992}\n",
      "{'neg': 0.056, 'neu': 0.744, 'pos': 0.2, 'compound': 0.9876}\n",
      "{'neg': 0.046, 'neu': 0.788, 'pos': 0.166, 'compound': 0.992}\n",
      "{'neg': 0.073, 'neu': 0.767, 'pos': 0.16, 'compound': 0.9769}\n",
      "{'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.5013}\n",
      "{'neg': 0.058, 'neu': 0.835, 'pos': 0.107, 'compound': 0.98}\n",
      "{'neg': 0.044, 'neu': 0.729, 'pos': 0.227, 'compound': 0.9987}\n",
      "{'neg': 0.064, 'neu': 0.798, 'pos': 0.137, 'compound': 0.9906}\n",
      "{'neg': 0.049, 'neu': 0.796, 'pos': 0.155, 'compound': 0.9876}\n",
      "{'neg': 0.057, 'neu': 0.75, 'pos': 0.193, 'compound': 0.9972}\n",
      "{'neg': 0.028, 'neu': 0.849, 'pos': 0.123, 'compound': 0.9837}\n",
      "{'neg': 0.033, 'neu': 0.833, 'pos': 0.134, 'compound': 0.9914}\n",
      "{'neg': 0.019, 'neu': 0.801, 'pos': 0.18, 'compound': 0.9979}\n",
      "{'neg': 0.033, 'neu': 0.737, 'pos': 0.23, 'compound': 0.9989}\n",
      "{'neg': 0.066, 'neu': 0.622, 'pos': 0.312, 'compound': 0.9908}\n",
      "{'neg': 0.01, 'neu': 0.836, 'pos': 0.154, 'compound': 0.9829}\n",
      "{'neg': 0.062, 'neu': 0.749, 'pos': 0.189, 'compound': 0.9982}\n",
      "{'neg': 0.043, 'neu': 0.672, 'pos': 0.286, 'compound': 0.9993}\n",
      "{'neg': 0.039, 'neu': 0.715, 'pos': 0.246, 'compound': 0.9971}\n",
      "{'neg': 0.034, 'neu': 0.8, 'pos': 0.165, 'compound': 0.9758}\n",
      "{'neg': 0.034, 'neu': 0.823, 'pos': 0.143, 'compound': 0.9954}\n",
      "{'neg': 0.111, 'neu': 0.695, 'pos': 0.194, 'compound': 0.9959}\n",
      "{'neg': 0.022, 'neu': 0.788, 'pos': 0.19, 'compound': 0.9926}\n",
      "{'neg': 0.084, 'neu': 0.765, 'pos': 0.151, 'compound': 0.9808}\n",
      "{'neg': 0.051, 'neu': 0.738, 'pos': 0.211, 'compound': 0.9927}\n",
      "{'neg': 0.036, 'neu': 0.845, 'pos': 0.119, 'compound': 0.9888}\n",
      "{'neg': 0.057, 'neu': 0.797, 'pos': 0.146, 'compound': 0.9796}\n",
      "{'neg': 0.041, 'neu': 0.754, 'pos': 0.204, 'compound': 0.9928}\n",
      "{'neg': 0.009, 'neu': 0.815, 'pos': 0.176, 'compound': 0.9911}\n",
      "{'neg': 0.073, 'neu': 0.699, 'pos': 0.227, 'compound': 0.9899}\n",
      "{'neg': 0.054, 'neu': 0.772, 'pos': 0.174, 'compound': 0.9899}\n",
      "{'neg': 0.109, 'neu': 0.776, 'pos': 0.115, 'compound': 0.3336}\n",
      "{'neg': 0.132, 'neu': 0.743, 'pos': 0.125, 'compound': -0.1531}\n",
      "{'neg': 0.208, 'neu': 0.556, 'pos': 0.237, 'compound': 0.0516}\n",
      "{'neg': 0.131, 'neu': 0.77, 'pos': 0.099, 'compound': -0.8689}\n",
      "{'neg': 0.021, 'neu': 0.732, 'pos': 0.247, 'compound': 0.9986}\n",
      "{'neg': 0.056, 'neu': 0.725, 'pos': 0.219, 'compound': 0.9976}\n",
      "{'neg': 0.06, 'neu': 0.857, 'pos': 0.083, 'compound': 0.4215}\n",
      "{'neg': 0.088, 'neu': 0.795, 'pos': 0.117, 'compound': 0.8944}\n",
      "{'neg': 0.066, 'neu': 0.798, 'pos': 0.136, 'compound': 0.9686}\n",
      "{'neg': 0.029, 'neu': 0.853, 'pos': 0.118, 'compound': 0.9839}\n",
      "{'neg': 0.047, 'neu': 0.836, 'pos': 0.117, 'compound': 0.9578}\n",
      "{'neg': 0.104, 'neu': 0.78, 'pos': 0.116, 'compound': 0.5638}\n",
      "{'neg': 0.069, 'neu': 0.77, 'pos': 0.161, 'compound': 0.997}\n",
      "{'neg': 0.052, 'neu': 0.732, 'pos': 0.216, 'compound': 0.9915}\n",
      "{'neg': 0.045, 'neu': 0.833, 'pos': 0.122, 'compound': 0.926}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                   stock  positives  negatives\n",
       "0                  Tesla          6          0\n",
       "1                 NVIDIA          5          0\n",
       "2             Alphabet A          5          0\n",
       "3   CrowdStrike Holdings          5          0\n",
       "4                  Apple          4          0\n",
       "5             Amazon.com          7          0\n",
       "6                Verizon          7          0\n",
       "7      AMC Entertainment          5          0\n",
       "8             Alphabet C          8          0\n",
       "9        Coinbase Global          4          0\n",
       "10  Super Micro Computer          8          0\n",
       "11                   AMD          3          0\n",
       "12             Microsoft          5          1\n",
       "13        Meta Platforms          4          2\n",
       "14                  Nike          7          0\n",
       "15           Alibaba ADR          7          0\n",
       "16             Nio A ADR          7          0\n",
       "17              Palantir          8          0\n",
       "18             Starbucks          6          0\n",
       "19           Lucid Group          6          0\n",
       "20                Micron          7          0\n",
       "21  Taiwan Semiconductor          8          0\n",
       "22              Broadcom          8          0\n",
       "23                 Intel          7          0\n",
       "24         GameStop Corp          5          0\n",
       "25         MicroStrategy          7          0\n",
       "26                Pfizer          4          0\n",
       "27                Boeing          7          0\n",
       "28           Walt Disney          5          2\n",
       "29               Netflix          8          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>positives</th>\n",
       "      <th>negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alphabet A</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrowdStrike Holdings</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Verizon</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AMC Entertainment</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alphabet C</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Coinbase Global</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Super Micro Computer</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AMD</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Meta Platforms</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Nike</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alibaba ADR</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nio A ADR</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Palantir</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lucid Group</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Micron</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Taiwan Semiconductor</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Broadcom</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Intel</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GameStop Corp</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MicroStrategy</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pfizer</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Boeing</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Walt Disney</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:49:08.735863Z",
     "start_time": "2024-08-11T18:49:08.263841Z"
    }
   },
   "cell_type": "code",
   "source": "clear_db()",
   "id": "541694361eea29e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database cleared successfully.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
